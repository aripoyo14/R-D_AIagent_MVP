"""
DuckDuckGoã‚’ä½¿ç”¨ã—ãŸç‰¹è¨±æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹
"""
import logging
from ddgs import DDGS
from typing import List, Dict, Union
import requests
from bs4 import BeautifulSoup

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)


def fetch_patent_abstract(patent_url: str, timeout: int = 10) -> str:
    """
    Google Patentsã®URLã‹ã‚‰ç‰¹è¨±ã®è¦ç´„ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        patent_url: Google Patentsã®URL
        timeout: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    
    Returns:
        str: ç‰¹è¨±ã®è¦ç´„ï¼ˆå–å¾—ã§ããªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(patent_url, headers=headers, timeout=timeout)
        
        if response.status_code != 200:
            logger.warning(f"ç‰¹è¨±URLã®å–å¾—ã«å¤±æ•—: {patent_url} (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code})")
            return ""
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Google Patentsã®è¦ç´„ã‚’æ¢ã™ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™ï¼‰
        abstract = None
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: abstractã‚¯ãƒ©ã‚¹ã‚’æŒã¤div
        abstract = soup.find('div', {'class': 'abstract'})
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: itemprop="description"ã‚’æŒã¤section
        if not abstract:
            abstract = soup.find('section', {'itemprop': 'description'})
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: abstractã‚’å«ã‚€ã‚¯ãƒ©ã‚¹å
        if not abstract:
            abstract_elements = soup.find_all(['div', 'section'], class_=lambda x: x and 'abstract' in str(x).lower())
            if abstract_elements:
                abstract = abstract_elements[0]
        
        if abstract:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            abstract_text = abstract.get_text(separator=' ', strip=True)
            # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            abstract_text = ' '.join(abstract_text.split())
            return abstract_text
        else:
            logger.warning(f"è¦ç´„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {patent_url}")
            return ""
            
    except requests.exceptions.Timeout:
        logger.warning(f"ç‰¹è¨±URLã®å–å¾—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ: {patent_url}")
        return ""
    except requests.exceptions.RequestException as e:
        logger.warning(f"ç‰¹è¨±URLã®å–å¾—ã‚¨ãƒ©ãƒ¼: {patent_url} - {str(e)}")
        return ""
    except Exception as e:
        logger.warning(f"è¦ç´„å–å¾—ã‚¨ãƒ©ãƒ¼: {patent_url} - {str(e)}")
        return ""

def search_patents(keywords: Union[str, List[str]], max_results: int = 5, debug: bool = False) -> str:
    """
    DuckDuckGoã‚’ä½¿ç”¨ã—ã¦Google Patents (site:patents.google.com) ã‹ã‚‰ç‰¹è¨±ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
    
    Args:
        keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆï¼‰
        max_results: å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆè©³ç´°ãªãƒ­ã‚°ã‚’å‡ºåŠ›ï¼‰
        
    Returns:
        str: DuckDuckGoã‹ã‚‰ã®æ¤œç´¢çµæœï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
    """
    try:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆæ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾ã€ãƒªã‚¹ãƒˆã®å ´åˆã¯çµåˆï¼‰
        if isinstance(keywords, str):
            query_str = keywords
        else:
            query_str = " ".join(keywords)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ï¼‰
        query_str = " ".join(query_str.split())
        
        # æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰
        import re
        has_japanese = bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', query_str))
        
        # æ‹¬å¼§ã‚’å«ã‚€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡¦ç†
        # ä¾‹ï¼šã€Œ(ãƒ©ã‚¤ãƒ³å¹…ãƒ©ãƒ•ãƒã‚¹)ã€â†’ã€Œãƒ©ã‚¤ãƒ³å¹…ãƒ©ãƒ•ãƒã‚¹ã€ã€ã€ŒEUV (æ¥µç«¯ç´«å¤–ç·š)ã€â†’ã€ŒEUVã€
        def clean_keyword(kw):
            """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ‹¬å¼§ã¨ãã®ä¸­èº«ã‚’é™¤å»"""
            # æ‹¬å¼§ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’é™¤å»ï¼ˆä¾‹ï¼šã€Œ(ãƒ©ã‚¤ãƒ³å¹…ãƒ©ãƒ•ãƒã‚¹)ã€â†’ã€Œã€ï¼‰
            kw = re.sub(r'\([^)]*\)', '', kw)
            # æ‹¬å¼§ã®ã¿ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»
            kw = kw.strip()
            # ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯Noneã‚’è¿”ã™
            return kw if kw else None
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†å‰²ã—ã¦ã€æ‹¬å¼§ã‚’é™¤å»
        keywords_list = []
        for kw in query_str.split():
            cleaned = clean_keyword(kw)
            if cleaned:
                keywords_list.append(cleaned)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆãŒç©ºã®å ´åˆã¯å…ƒã®æ–‡å­—åˆ—ã‚’ä½¿ç”¨
        if not keywords_list:
            keywords_list = query_str.split()
        keyword_count = len(keywords_list)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå¤šã™ãã‚‹å ´åˆï¼ˆ15å€‹ä»¥ä¸Šï¼‰ã€ä¸»è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’ä½¿ç”¨
        # è‹±èªç•¥èªï¼ˆå¤§æ–‡å­—ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼‰ã¨é‡è¦ãªæŠ€è¡“ç”¨èªã‚’å„ªå…ˆ
        MAX_KEYWORDS = 10  # æ¤œç´¢ã‚¯ã‚¨ãƒªã«ä½¿ç”¨ã™ã‚‹æœ€å¤§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°
        
        if keyword_count > MAX_KEYWORDS:
            # æŠ€è¡“ç”¨èªã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
            # é‡è¦ãªæŠ€è¡“ç”¨èªã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆææ–™ã€ãƒ—ãƒ­ã‚»ã‚¹ã€ç”¨é€”ãªã©ï¼‰
            technical_patterns = [
                r'.*æ¨¹è„‚$', r'.*ææ–™$', r'.*æˆå½¢$', r'.*ãƒ—ãƒ­ã‚»ã‚¹$', r'.*æ–¹æ³•$',
                r'.*æ¥ç€å‰¤$', r'.*è¤‡åˆæ$', r'.*å¼·åŒ–$', r'.*ç¹Šç¶­$', r'.*é›»æ± $',
                r'.*é›»è§£è³ª$', r'.*å›ºä½“$', r'.*æ¶²ä½“$', r'.*ã‚¬ã‚¹$', r'.*è†œ$',
                r'.*ã‚³ãƒ¼ãƒ†ã‚£ãƒ³ã‚°$', r'.*å‡¦ç†$', r'.*è£½é€ $', r'.*åˆæˆ$', r'.*åˆ†è§£$',
                r'.*åå¿œ$', r'.*è§¦åª’$', r'.*æ·»åŠ å‰¤$', r'.*å……å¡«æ$', r'.*æ”¹è³ª$',
                r'.*ç¡¬åŒ–$', r'.*æ¶æ©‹$', r'.*é‡åˆ$', r'.*å…±é‡åˆ$', r'.*ãƒ–ãƒ­ãƒƒã‚¯å…±é‡åˆ$',
                r'.*ã‚°ãƒ©ãƒ•ãƒˆå…±é‡åˆ$', r'.*ãƒ©ãƒ³ãƒ€ãƒ å…±é‡åˆ$', r'.*äº¤äº’å…±é‡åˆ$',
                r'.*ãƒ¬ã‚¸ã‚¹ãƒˆ$', r'.*ãƒ•ã‚©ãƒˆãƒ¬ã‚¸ã‚¹ãƒˆ$', r'.*ãƒªã‚½ã‚°ãƒ©ãƒ•ã‚£$',  # åŠå°ä½“é–¢é€£
                r'.*åŠå°ä½“$', r'.*ãƒ—ãƒ­ã‚»ã‚¹ãƒãƒ¼ãƒ‰$', r'.*å¾®ç´°åŠ å·¥$',  # åŠå°ä½“è£½é€ 
            ]
            
            # ä¸€èˆ¬çš„ãªå½¢å®¹è©çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè£œåŠ©çš„ã«ä½¿ç”¨ï¼‰
            # ã“ã‚Œã‚‰ã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹ãŒã€å®Œå…¨ã«ã¯é™¤å¤–ã—ãªã„
            auxiliary_patterns = [
                r'^é«˜', r'^ä½', r'^å„ª', r'^è‰¯', r'^å¼·', r'^å¼±',
                r'^è€', r'^æŠ—', r'^é˜²', r'^é€Ÿ', r'^ç·©', r'^æ€¥',
                r'æ€§$', r'åº¦$', r'ç‡$', r'æ¯”$', r'å€¤$',  # æ€§è³ªã‚’è¡¨ã™æ¥å°¾è¾
            ]
            
            def calculate_keyword_priority(kw):
                """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å„ªå…ˆåº¦ã‚’è¨ˆç®—ï¼ˆé«˜ã„ã»ã©é‡è¦ï¼‰"""
                priority = 0
                
                # è‹±èªç•¥èªï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
                if re.match(r'^[A-Z]{2,}$', kw):
                    return 1000 + len(kw)  # é•·ã„ç•¥èªã»ã©é‡è¦
                
                # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å ´åˆ
                if has_japanese and re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', kw):
                    # æŠ€è¡“ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹å ´åˆï¼ˆé«˜å„ªå…ˆåº¦ï¼‰
                    for pattern in technical_patterns:
                        if re.match(pattern, kw):
                            priority += 500
                            break
                    
                    # è¤‡åˆèªï¼ˆé•·ã„å˜èªï¼‰ã‚’å„ªå…ˆ
                    priority += len(kw) * 10
                    
                    # æ¼¢å­—ã‚’å«ã‚€è¤‡åˆèªã‚’å„ªå…ˆï¼ˆã‚ˆã‚Šå°‚é–€çš„ï¼‰
                    if re.search(r'[\u4E00-\u9FAF]', kw):
                        priority += 50
                    
                    # ã‚«ã‚¿ã‚«ãƒŠã®ã¿ã®å˜èªã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹ï¼ˆä¸€èˆ¬çš„ãªç”¨èªã®å¯èƒ½æ€§ï¼‰
                    if re.match(r'^[\u30A0-\u30FF]+$', kw):
                        priority -= 20
                    
                    # è£œåŠ©çš„ãªå½¢å®¹è©çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹
                    for pattern in auxiliary_patterns:
                        if re.match(pattern, kw):
                            priority -= 100
                            break
                    
                    # çŸ­ã™ãã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ2æ–‡å­—ä»¥ä¸‹ï¼‰ã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹
                    if len(kw) <= 2:
                        priority -= 50
                
                # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨æ•°å­—ï¼‰
                elif re.match(r'^[A-Za-z0-9]+$', kw):
                    priority += 200 + len(kw) * 5
                
                return priority
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            keywords_with_priority = [
                (kw, calculate_keyword_priority(kw))
                for kw in keywords_list
            ]
            keywords_with_priority.sort(key=lambda x: x[1], reverse=True)
            
            # å„ªå…ˆåº¦ã®é«˜ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ
            priority_keywords = [kw for kw, _ in keywords_with_priority[:MAX_KEYWORDS]]
            
            optimized_query_str = " ".join(priority_keywords)
            
            if debug:
                print(f"[DEBUG] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ãŒå¤šã™ãã‚‹ãŸã‚æœ€é©åŒ–: {keyword_count}å€‹ â†’ {len(priority_keywords)}å€‹")
                print(f"[DEBUG] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å„ªå…ˆåº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½{min(MAX_KEYWORDS, len(keywords_with_priority))}å€‹ï¼‰:")
                for i, (kw, priority) in enumerate(keywords_with_priority[:MAX_KEYWORDS], 1):
                    print(f"[DEBUG]   {i}. {kw} (å„ªå…ˆåº¦: {priority})")
                print(f"[DEBUG] æœ€é©åŒ–å¾Œã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {optimized_query_str}")
        else:
            optimized_query_str = query_str
        
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
        # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆã€
        # è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è©¦ã™ã“ã¨ã§æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹
        queries = []
        seen_queries = set()  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚
        
        def add_query_if_unique(query):
            """é‡è¤‡ã—ãªã„ã‚¯ã‚¨ãƒªã®ã¿ã‚’è¿½åŠ """
            if query and query not in seen_queries:
                seen_queries.add(query)
                queries.append(query)
        
        if has_japanese:
            # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆ
            # 1. è‹±èªç•¥èªã®ã¿ã®ã‚¯ã‚¨ãƒªï¼ˆçŸ­ãã€å…·ä½“çš„ï¼‰- æœ€å„ªå…ˆ
            english_only = " ".join([w for w in optimized_query_str.split() if re.match(r'^[A-Za-z0-9]+$', w)])
            if english_only:
                add_query_if_unique(f"{english_only} ç‰¹è¨±")
            
            # 2. ä¸»è¦ãªæŠ€è¡“ç”¨èªã®ã¿ï¼ˆæ—¥æœ¬èªã®åè©ã‚’æŠ½å‡ºã€3-5å€‹ã«åˆ¶é™ï¼‰
            # ã€Œç‰¹è¨±ã€ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è‡ªä½“ã¯é™¤å¤–ï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªã«è¿½åŠ ã™ã‚‹ãŸã‚ï¼‰
            japanese_words = [
                w for w in optimized_query_str.split()
                if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', w) and w != "ç‰¹è¨±"
            ]
            
            if japanese_words:
                # æ‹¬å¼§ã‚’å«ã‚€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ‹¬å¼§ã‚’é™¤å»
                japanese_words = [clean_keyword(w) for w in japanese_words if clean_keyword(w)]
                # 3-5å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆ¶é™ï¼ˆé•·ã™ãã‚‹ã‚¯ã‚¨ãƒªã‚’é¿ã‘ã‚‹ï¼‰
                if len(japanese_words) > 5:
                    # é•·ã„å˜èªã‚’å„ªå…ˆï¼ˆã‚ˆã‚Šå…·ä½“çš„ãªæŠ€è¡“ç”¨èªï¼‰
                    japanese_words_sorted = sorted(japanese_words, key=len, reverse=True)
                    japanese_only = " ".join(japanese_words_sorted[:5])
                else:
                    japanese_only = " ".join(japanese_words)
                if japanese_only:
                    add_query_if_unique(f"{japanese_only} ç‰¹è¨±")
            
            # 3. è‹±èªç•¥èª + ä¸»è¦ãªæ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ2-3å€‹ï¼‰ã®çµ„ã¿åˆã‚ã›
            if english_only and japanese_words:
                if len(japanese_words) >= 2:
                    # æœ€åˆã®2-3å€‹ã®æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                    short_japanese = " ".join(japanese_words[:3])
                    combined_query = f"{english_only} {short_japanese} ç‰¹è¨±"
                    # ã‚¯ã‚¨ãƒª1ã‚„ã‚¯ã‚¨ãƒª2ã¨é‡è¤‡ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
                    if combined_query != f"{english_only} ç‰¹è¨±" and combined_query != f"{japanese_only} ç‰¹è¨±":
                        add_query_if_unique(combined_query)
            
            # 4. æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¨ä½“ï¼ˆæ‹¬å¼§ã‚’é™¤å»ã—ã¦ã‹ã‚‰ä½¿ç”¨ã€ã€Œç‰¹è¨±ã€ã‚’é™¤å¤–ï¼‰
            optimized_words = [
                clean_keyword(w) for w in optimized_query_str.split()
                if clean_keyword(w) and clean_keyword(w) != "ç‰¹è¨±"
            ]
            optimized_cleaned = " ".join(optimized_words)
            if optimized_cleaned and len(optimized_cleaned.split()) <= 8:  # 8å€‹ä»¥ä¸‹ã«åˆ¶é™
                final_query = f"{optimized_cleaned} ç‰¹è¨±"
                # æ—¢å­˜ã®ã‚¯ã‚¨ãƒªã¨é‡è¤‡ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
                if final_query not in seen_queries:
                    add_query_if_unique(final_query)
        else:
            # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã®å ´åˆ
            add_query_if_unique(f"{optimized_query_str} patent")
            add_query_if_unique(f"{optimized_query_str} patent 2024 2025")
        
        logger.info(f"ğŸ” ç‰¹è¨±æ¤œç´¢é–‹å§‹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={optimized_query_str}, æ¤œç´¢ã‚¯ã‚¨ãƒªæ•°={len(queries)}, max_results={max_results}")
        if debug:
            print(f"[DEBUG] ğŸ” ç‰¹è¨±æ¤œç´¢é–‹å§‹")
            print(f"[DEBUG] å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query_str}")
            print(f"[DEBUG] æœ€é©åŒ–å¾Œã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {optimized_query_str}")
            print(f"[DEBUG] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {keyword_count}å€‹")
            print(f"[DEBUG] æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {has_japanese}")
            print(f"[DEBUG] æ¤œç´¢ã‚¯ã‚¨ãƒªæ•°: {len(queries)}")
            for i, q in enumerate(queries, 1):
                print(f"[DEBUG]  ã‚¯ã‚¨ãƒª {i}: {q}")
            print(f"[DEBUG] æœ€å¤§å–å¾—ä»¶æ•°: {max_results}")
        
        results_list = []
        result_count = 0
        seen_urls = set()  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚
        
        with DDGS() as ddgs:
            # å„ã‚¯ã‚¨ãƒªã‚’é †ç•ªã«è©¦ã™
            for query_idx, query in enumerate(queries, 1):
                if result_count >= max_results:
                    break
                
                if debug:
                    print(f"[DEBUG] ã‚¯ã‚¨ãƒª {query_idx}/{len(queries)} ã‚’å®Ÿè¡Œä¸­...")
                
                # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å ´åˆã¯ç‰¹è¨±ã‚µã‚¤ãƒˆã®çµæœãŒå°‘ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
                # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆmax_results * 5ï¼‰
                # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å ´åˆã¯ max_results * 3 ã§ååˆ†
                fetch_limit = max_results * 5 if has_japanese else max_results * 3
                
                try:
                    total_fetched = 0
                    patents_found = 0
                    all_urls = []  # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå–å¾—ã—ãŸURLã®ãƒªã‚¹ãƒˆ
                    
                    # DuckDuckGoã®æ¤œç´¢ã‚’å®Ÿè¡Œ
                    search_results = ddgs.text(query, max_results=fetch_limit)
                    
                    for r in search_results:
                        total_fetched += 1
                        href = r.get("href", "")
                        title = r.get("title", "")
                        all_urls.append(href)
                        
                        if debug and total_fetched <= 3:
                            print(f"[DEBUG]   å–å¾—çµæœ #{total_fetched}: {title[:60]}...")
                            print(f"[DEBUG]      URL: {href}")
                        
                        # patents.google.comã®URLã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã€é‡è¤‡ã‚’é¿ã‘ã‚‹
                        if "patents.google.com" in href and href not in seen_urls:
                            seen_urls.add(href)
                            patents_found += 1
                            result_count += 1
                            
                            # Google Patentsã‹ã‚‰è¦ç´„ã‚’å–å¾—
                            if debug:
                                print(f"[DEBUG] æ¤œç´¢çµæœ #{result_count} (ã‚¯ã‚¨ãƒª{query_idx}, ç·å–å¾—: {total_fetched}ä»¶):")
                                print(f"[DEBUG]  ã‚¿ã‚¤ãƒˆãƒ«: {r['title']}")
                                print(f"[DEBUG]  URL: {href}")
                                print(f"[DEBUG]  ã‚¹ãƒ‹ãƒšãƒƒãƒˆ: {r['body'][:100]}...")
                                print(f"[DEBUG]  Google Patentsã‹ã‚‰è¦ç´„ã‚’å–å¾—ä¸­...")
                            
                            abstract = fetch_patent_abstract(href)
                            
                            # çµæœãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆè¦ç´„ãŒã‚ã‚Œã°å«ã‚ã‚‹ï¼‰
                            result_text = f"Title: {r['title']}\nURL: {href}\n"
                            if abstract:
                                result_text += f"Abstract: {abstract}\n"
                            else:
                                # è¦ç´„ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ä½¿ç”¨
                                result_text += f"Snippet: {r['body']}\n"
                            
                            results_list.append(result_text)
                            
                            if debug:
                                if abstract:
                                    print(f"[DEBUG]  è¦ç´„: {abstract[:200]}...")
                                else:
                                    print(f"[DEBUG]  è¦ç´„: å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰")
                            
                            # å¿…è¦ãªä»¶æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
                            if result_count >= max_results:
                                break
                    
                    if debug:
                        print(f"[DEBUG] ã‚¯ã‚¨ãƒª {query_idx}: ç·å–å¾—ä»¶æ•°={total_fetched}ä»¶, ç‰¹è¨±ã‚µã‚¤ãƒˆã®çµæœ={patents_found}ä»¶")
                        if total_fetched == 0:
                            print(f"[DEBUG] âš ï¸ è­¦å‘Š: æ¤œç´¢çµæœãŒ0ä»¶ã§ã™ã€‚DuckDuckGoã®æ¤œç´¢APIãŒåˆ©ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                        elif patents_found == 0 and total_fetched > 0:
                            print(f"[DEBUG] âš ï¸ è­¦å‘Š: æ¤œç´¢çµæœã¯{total_fetched}ä»¶ã‚ã‚Šã¾ã—ãŸãŒã€ç‰¹è¨±ã‚µã‚¤ãƒˆã®çµæœãŒ0ä»¶ã§ã™ã€‚")
                            if total_fetched <= 5:
                                print(f"[DEBUG]   å–å¾—ã—ãŸURLã®ä¾‹: {all_urls[:3]}")
                        
                except Exception as e:
                    if debug:
                        print(f"[DEBUG] ã‚¯ã‚¨ãƒª {query_idx} ã§ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)}")
                        import traceback
                        traceback.print_exc()
                    continue
            
            if debug:
                print(f"[DEBUG] æœ€çµ‚çµæœ: {result_count}ä»¶ã®ç‰¹è¨±ã‚’å–å¾—")
        
        if not results_list:
            logger.warning(f"âš ï¸ ç‰¹è¨±æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: ã‚¯ã‚¨ãƒª={query}")
            if debug:
                print(f"[DEBUG] âš ï¸ ç‰¹è¨±æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return "ç‰¹è¨±æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        logger.info(f"âœ… ç‰¹è¨±æ¤œç´¢å®Œäº†: {result_count}ä»¶ã®çµæœã‚’å–å¾—")
        if debug:
            print(f"[DEBUG] âœ… ç‰¹è¨±æ¤œç´¢å®Œäº†: {result_count}ä»¶ã®çµæœã‚’å–å¾—")
            
        return "\n".join(results_list)
    except Exception as e:
        error_msg = f"ç‰¹è¨±æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"
        logger.error(f"âŒ {error_msg}", exc_info=True)
        if debug:
            print(f"[DEBUG] âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
            import traceback
            traceback.print_exc()
        return error_msg
