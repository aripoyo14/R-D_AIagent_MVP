"""ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†éšŠï¼ˆ5ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰ã®ä¸­æ ¸ãƒ­ã‚¸ãƒƒã‚¯ã€‚

Streamlitã®ãƒãƒ£ãƒƒãƒˆUIã§5äººãŒè­°è«–ã™ã‚‹ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨é€²è¡Œã¯ä»•æ§˜ã«å¾“ã†ã€‚
"""

import os
from typing import List, Optional, Dict

import streamlit as st
from langchain_core.messages import HumanMessage, SystemMessage

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    ChatGoogleGenerativeAI = None

import backend
from services.patents import search_patents
from services.academic import search_arxiv, format_arxiv_results
from services.ai_review import select_important_tags

from services.report_generator import REPORT_SYSTEM_PROMPT, REPORT_HUMAN_PROMPT
from components.conversation_log import get_chat_css, render_message_html

# å®šæ•°å®šç¾©
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ORCHESTRATOR_AVATAR = os.path.join(BASE_DIR, "images", "Orchestrator.png")
MARKET_RESEARCHER_AVATAR = os.path.join(BASE_DIR, "images", "Market_Researcher.png")
INTERNAL_SPECIALIST_AVATAR = os.path.join(BASE_DIR, "images", "Internal_Specialist.png")
SOLUTION_ARCHITECT_AVATAR = os.path.join(BASE_DIR, "images", "Solution_Architect.png")
DEVILS_ADVOCATE_AVATAR = os.path.join(BASE_DIR, "images", "Devils_Advocate.png")

# å¸‚å ´è¦æ¨¡ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ç«¶åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ï¼ˆæ¤œç´¢çµæœãŒä¸ååˆ†ãªå ´åˆã«ä½¿ç”¨ï¼‰
FALLBACK_MARKET_INFO = """## å¸‚å ´è¦æ¨¡ï¼ˆMarket Sizeï¼‰

EVç”¨ç†±ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ å¸‚å ´ã¯2024â€“2030å¹´ã«CAGR 20%ä»¥ä¸Šã§æˆé•·
â†’ EVæ™®åŠã«ä¼´ã„ã€ãƒãƒƒãƒ†ãƒªãƒ¼ç†±ç®¡ç†ã®é‡è¦æ€§ãŒé«˜ã¾ã‚Šã€å¸‚å ´å…¨ä½“ãŒæ€¥æ‹¡å¤§ã€‚

éƒ¨æå˜ä¾¡ãŒé«˜ãã€é«˜ä»˜åŠ ä¾¡å€¤å¸‚å ´

æ”¾ç†±æ¨¹è„‚ã¯é€šå¸¸ã®ã‚¨ãƒ³ãƒ—ãƒ©ã® 3ã€œ5å€ï¼ˆ1,500ã€œ2,500å††/kgï¼‰ ã®ä¾¡æ ¼å¸¯ã€‚

æ—­æ—¥è‡ªå‹•è»Šã®äº‹ä¾‹ã§ã¯ï¼š

å¹´é–“20ä¸‡å° Ã— 1å°12å€‹ = 240ä¸‡å€‹/å¹´

1å€‹200gã¨ä»®å®š â†’ å¹´é–“ç´„480ãƒˆãƒ³ã®ã‚³ãƒ³ãƒ‘ã‚¦ãƒ³ãƒ‰éœ€è¦
â†’ OEM 1ç¤¾ã ã‘ã§ã‚‚æ•°ç™¾ãƒˆãƒ³è¦æ¨¡ã®éœ€è¦ãŒç”Ÿã¾ã‚Œã‚‹ã€æˆé•·ä½™åœ°ã®å¤§ãã„é ˜åŸŸã€‚

## ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆMarket Trendsï¼‰

é‡‘å±ã‹ã‚‰æ¨¹è„‚ã¸ã®ç½®æ›ãŒæ€¥åŠ é€Ÿ

ãƒãƒƒãƒ†ãƒªãƒ¼ãƒã‚¦ã‚¸ãƒ³ã‚°ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚«ãƒãƒ¼ã«ãŠã„ã¦ã€

è»½é‡åŒ–ï¼ˆæ¯”é‡ï¼šã‚¢ãƒ«ãƒŸ2.7 â†’ æ¨¹è„‚1.6ã€œ1.8ï¼‰

ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼ˆå¾ŒåŠ å·¥ãƒ¬ã‚¹ã€æˆå½¢æ€§å‘ä¸Šï¼‰
ã‚’ç›®çš„ã«ã€ã‚¢ãƒ«ãƒŸ â†’ æ”¾ç†±æ¨¹è„‚ã¸ã®è»¢æ› ãŒé€²ã‚€ã€‚

EVé«˜é›»åœ§åŒ–ã«ä¼´ã†ææ–™è¦æ±‚ã®é«˜åº¦åŒ–

è€é›»åœ§ã€ç†±ä¼å°ç‡ã€é›£ç‡ƒæ€§ã€è€ç†±æ€§ãªã©ã€ä»•æ§˜ãŒæ€¥é€Ÿã«é«˜ãƒ¬ãƒ™ãƒ«åŒ–ã€‚

ã“ã‚Œã«ã‚ˆã‚Šã€PPSãƒ»PPAãƒ»é«˜æ©Ÿèƒ½PBTãªã©ã®é«˜æ€§èƒ½æ¨¹è„‚ãŒæ¡ç”¨æ‹¡å¤§ä¸­ã€‚

ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£å¯¾å¿œã®å¼·åŒ–

æ¬§å· OEM ã‚’ä¸­å¿ƒã« ãƒã‚¤ã‚ªãƒã‚¹ã€ãƒªã‚µã‚¤ã‚¯ãƒ«æã€LCAå¯¾å¿œ ã¸ã®è¦æ±‚ãŒå¢—åŠ ã€‚

æ¨¹è„‚ææ–™ãƒ¡ãƒ¼ã‚«ãƒ¼å´ã‚‚ã€Œãƒã‚¤ã‚ªãƒã‚¹PPAã€ã€Œä½COâ‚‚æ’å‡ºææ–™ã€ã‚’å¼·åŒ–ã™ã‚‹å‚¾å‘ã€‚

## ç«¶åˆï¼ˆCompetitive Landscapeï¼‰

æ±ãƒ¬ï¼ˆTorayï¼‰

å¼·ã¿ï¼šPPSã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒƒãƒ—ãƒ¡ãƒ¼ã‚«ãƒ¼ã€‚é‡åˆã€œã‚³ãƒ³ãƒ‘ã‚¦ãƒ³ãƒ‰ã¾ã§å‚ç›´çµ±åˆã€‚

å®Ÿç¸¾ï¼šã€Œãƒˆãƒ¬ãƒªãƒŠã€ã§é«˜æ”¾ç†±ã‚°ãƒ¬ãƒ¼ãƒ‰ã€é«˜CTIã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ—¢ã«å±•é–‹ã€‚

è„…å¨ï¼šä¾›çµ¦åŠ›ãƒ»ä¾¡æ ¼ç«¶äº‰åŠ›ã¨ã‚‚ã«å¼·ãã€æœ€é‡è¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

ãƒãƒªãƒ—ãƒ©ã‚¹ãƒãƒƒã‚¯ã‚¹ï¼ˆPolyplasticsï¼‰

å¼·ã¿ï¼šPBTãƒ»PPSã«å¼·ãã€è‡ªå‹•è»Šç”¨é€”ã§é«˜ã„ã‚·ã‚§ã‚¢ã€‚

ç‰¹å¾´ï¼šé‡‘å±ã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆå½¢ãƒ»æ¥åˆæŠ€è¡“ï¼ˆAKITï¼‰ã«å„ªã‚Œã‚‹ã€‚

å‹•å‘ï¼šãƒã‚¹ãƒãƒ¼å‘¨è¾ºå‘ã‘ã«è€ãƒ’ãƒ¼ãƒˆã‚·ãƒ§ãƒƒã‚¯æ€§å‘ä¸Šã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’å±•é–‹ã€‚

ã‚½ãƒ«ãƒ™ã‚¤ï¼ˆSolvayï¼‰

å¼·ã¿ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã®é«˜æ©Ÿèƒ½PPAï¼ˆã‚¢ãƒ¢ãƒ‡ãƒ«ï¼‰ã¨PPSï¼ˆãƒ©ã‚¤ãƒˆãƒ³ï¼‰ã‚’ä¿æœ‰ã€‚

å·®åˆ¥åŒ–ï¼šè¶…é«˜é›»åœ§ã‚„æ¥µé™ç’°å¢ƒå‘ã‘ã®é«˜æ©Ÿèƒ½ææ–™ãŒè±Šå¯Œã€‚

å®Ÿç¸¾ï¼šæ¬§å·OEMã®æ¡ç”¨ä¾‹ãŒå¤šãã€ãƒ—ãƒ¬ãƒŸã‚¢ãƒ é ˜åŸŸã«å¼·ã„ã€‚

DSMï¼ˆEnvaliorï¼‰

å¼·ã¿ï¼šPA46ã‚„PPAï¼ˆForTiiï¼‰ã«åŠ ãˆã€ãƒã‚¤ã‚ªãƒã‚¹å¯¾å¿œï¼ˆEcoPaXXï¼‰ ã‚’ç©æ¥µå±•é–‹ã€‚

ç‰¹å¾´ï¼šã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ã®è¦æ±‚ãŒå¼·ã„OEMã¸ã®ç›¸æ€§ãŒé«˜ã„ã€‚

è„…å¨ï¼šæ—­æ—¥è‡ªå‹•è»ŠãŒã€Œãƒã‚¤ã‚ªãƒã‚¹è¦æœ›ã€ã‚’å‡ºã—ã¦ã„ã‚‹ãŸã‚ã€ç«¶åˆã¨ã—ã¦ç‰¹ã«å¼·ã„ã€‚
"""


def get_llm(temperature: float = 0.3, streaming: bool = False, model_name: str = "gemini-2.5-flash-lite"):
    """LLMã‚’è¿”ã™ãƒ•ã‚¡ã‚¯ãƒˆãƒªã€‚Gemini 2.5 Flash ã‚’ä½¿ç”¨ã€‚"""

    if ChatGoogleGenerativeAI is None:
        raise ImportError("Gemini ã‚’ä½¿ã†ã«ã¯ langchain-google-genai ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™")

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    return ChatGoogleGenerativeAI(
        # model="gemini-2.5-flash",
        model=model_name,
        temperature=temperature,
        google_api_key=api_key,
        streaming=streaming,
    )


def generate_orchestrator_brief(interview_memo: str, model_name: str = "gemini-2.5-flash-lite") -> str:
    """ğŸ‘‘å¸ä¼šç”¨ã®çŸ­ã„ãƒ–ãƒªãƒ¼ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    llm = get_llm(temperature=0.5, model_name=model_name)
    prompt = (
        "ã‚ãªãŸã¯ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®é¢è«‡ãƒ¡ãƒ¢ã‚’èª­ã¿ã€1æ®µè½ã§å¸ä¼šç”¨ãƒ–ãƒªãƒ¼ãƒ•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        "å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
        "å«ã‚ã‚‹è¦ç´ : ä¸»èª²é¡Œ/è¦æ±‚ã‚¹ãƒšãƒƒã‚¯ã€ç«¶åˆãƒ»ææ–™ã®å€™è£œã€ä¸»è¦ãƒªã‚¹ã‚¯ã€ç´æœŸãŒã‚ã‚Œã°æ˜ç¤ºã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®æŒ‡ç¤º"
        " (Market=äº‹å®Ÿèª¿æŸ», Internal=ç¤¾å†…çŸ¥è¦‹, Architect=ç™ºæƒ³, Devil=ãƒªã‚¹ã‚¯ç¢ºèª)ã€‚"
        f"\n\né¢è«‡ãƒ¡ãƒ¢:\n{interview_memo}"
    )
    response = llm.invoke([HumanMessage(content=prompt)])
    return response.content.strip()



def agent_market_researcher(tech_tags: List[str], use_case: str = "", model_name: str = "gemini-2.5-flash-lite") -> tuple[str, List[Dict]]:
    """ğŸ•µï¸å¸‚å ´èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚DuckDuckGo ã§å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œç´¢ã€‚
    
    Returns:
        tuple[str, List[Dict]]: (å¸‚å ´èª¿æŸ»ã‚µãƒãƒªãƒ¼, å­¦è¡“è«–æ–‡æƒ…å ±ã®ãƒªã‚¹ãƒˆ)
    """

    # é‡è¦åº¦ã®é«˜ã„ã‚¿ã‚°ã‚’é¸å®šï¼ˆæœ€å¤§5ã¤ï¼‰
    selected_tags = select_important_tags(tech_tags, interview_memo=use_case, max_tags=5, model_name=model_name)
    
    # é¸å®šã•ã‚ŒãŸã‚¿ã‚°ã§æ¤œç´¢ã‚’å®Ÿè¡Œ
    results = backend.search_market_trends(selected_tags, use_case) or ""
    patents = search_patents(selected_tags) or ""
    academics_list = search_arxiv(" ".join(selected_tags))
    academics = format_arxiv_results(academics_list) if academics_list else ""
    avatar = MARKET_RESEARCHER_AVATAR
    
    # æ¤œç´¢çµæœãŒç©ºã¾ãŸã¯ä¸ååˆ†ãªå ´åˆã®åˆ¤å®š
    # å¸‚å ´æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€ã¾ãŸã¯å¸‚å ´è¦æ¨¡ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ç«¶åˆã®æƒ…å ±ãŒä¸ååˆ†ãªå ´åˆ
    market_info_insufficient = (
        not results.strip() or 
        "å¸‚å ´æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ" in results or 
        "å¸‚å ´èª¿æŸ»çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ" in results
    )
    
    # æ¤œç´¢çµæœãŒç©ºã®å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ã‚’ä½¿ç”¨
    if not any([results.strip(), patents, academics]) or market_info_insufficient:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦å¸‚å ´èª¿æŸ»ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
        prompt = (
            "You are a Market Researcher. Summarize the following fallback market information "
            "into facts only (Competitors, Market Size, Trends, Patents, Academic papers). "
            "Respond in Japanese only.\n"
            "å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…ãšè¦‹å‡ºã—è¡Œã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„: '## ç«¶åˆä»–ç¤¾', '## å¸‚å ´è¦æ¨¡', '## ãƒˆãƒ¬ãƒ³ãƒ‰', '## ç‰¹è¨±', '## å­¦è¡“è«–æ–‡'.\n"
            "1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n"
            "Fallback Market Information:\n{fallback_info}\n\n"
            "Patents: {patents}\n\n"
            "Academic: {academics}\n\n"
            "æ³¨æ„: æ¤œç´¢çµæœãŒä¸ååˆ†ãªãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        ).format(
            fallback_info=FALLBACK_MARKET_INFO,
            patents=patents if patents else "ç‰¹è¨±æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            academics=academics if academics else "å­¦è¡“è«–æ–‡æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        )
        llm = get_llm(temperature=0.3, model_name=model_name)
        response = llm.invoke([HumanMessage(content=prompt)])
        summary = response.content.strip()
    else:
        # æ¤œç´¢çµæœãŒã‚ã‚‹å ´åˆã€é€šå¸¸ã®å‡¦ç†ã‚’å®Ÿè¡Œ
        # ãŸã ã—ã€å¸‚å ´è¦æ¨¡ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ç«¶åˆã®æƒ…å ±ãŒä¸ååˆ†ãªå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ã‚‚ä½µç”¨
        prompt = (
            "You are a Market Researcher. Summarize the following search results into facts only "
            "(Competitors, Market Size, Trends, Patents, Academic papers). No speculation. "
            "Respond in Japanese only.\n"
            "å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…ãšè¦‹å‡ºã—è¡Œã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„: '## ç«¶åˆä»–ç¤¾', '## å¸‚å ´è¦æ¨¡', '## ãƒˆãƒ¬ãƒ³ãƒ‰', '## ç‰¹è¨±', '## å­¦è¡“è«–æ–‡'.\n"
            "1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n"
            "Market Search Results: {results}\n\n"
            "Fallback Market Information (æ¤œç´¢çµæœãŒä¸ååˆ†ãªå ´åˆã«ä½¿ç”¨):\n{fallback_info}\n\n"
            "Patents: {patents}\n\n"
            "Academic: {academics}\n\n"
            "æ³¨æ„: æ¤œç´¢çµæœã«å¸‚å ´è¦æ¨¡ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ç«¶åˆã®æƒ…å ±ãŒä¸ååˆ†ãªå ´åˆã¯ã€"
            "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        ).format(
            results=results,
            fallback_info=FALLBACK_MARKET_INFO,
            patents=patents,
            academics=academics
        )
        llm = get_llm(temperature=0.3, model_name=model_name)
        response = llm.invoke([HumanMessage(content=prompt)])
        summary = response.content.strip()
    
    st.markdown(render_message_html("assistant", avatar, summary), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    if "conversation_log" in st.session_state:
        st.session_state.conversation_log.append({
            "role": "assistant",
            "avatar": avatar,
            "content": summary
        })
    return summary, academics_list



def agent_internal_specialist(query_text: str, department: str) -> tuple[str, List[dict]]:
    """ğŸ”ç¤¾å†…ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚ä»–äº‹æ¥­éƒ¨ã®çŸ¥è¦‹ã‚’æ¤œç´¢ã€‚"""

    hits = backend.search_cross_pollination(query_text, department, top_k=3) or []
    hits = backend.search_cross_pollination(query_text, department, top_k=3) or []
    avatar = INTERNAL_SPECIALIST_AVATAR
    if not hits:
        msg = "é–¢é€£ã™ã‚‹ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        st.markdown(render_message_html("assistant", avatar, msg), unsafe_allow_html=True)
        # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
        if "conversation_log" in st.session_state:
            st.session_state.conversation_log.append({
                "role": "assistant",
                "avatar": avatar,
                "content": msg
            })
        return msg, []

    bullet_lines = []
    for item in hits:
        metadata = item.get("metadata", {}) if isinstance(item, dict) else {}
        company = metadata.get("company") or metadata.get("client") or "Unknown Company"
        dept = metadata.get("department") or "Unknown Dept"
        content = item.get("content", "") if isinstance(item, dict) else ""
        bullet_lines.append(f"- {company} ({dept}): {content[:200]}".strip())

    result_text = "\n".join(bullet_lines)
    st.markdown(render_message_html("assistant", avatar, result_text), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    if "conversation_log" in st.session_state:
        st.session_state.conversation_log.append({
            "role": "assistant",
            "avatar": avatar,
            "content": result_text
        })
    return result_text, hits



def _stream_response(llm, messages: List, avatar: str) -> str:
    """LLMå‡ºåŠ›ã‚’Streamlitã«ã‚¹ãƒˆãƒªãƒ¼ãƒ è¡¨ç¤ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚"""

    buffer = ""
    placeholder = st.empty()
    for chunk in llm.stream(messages):
        if chunk.content:
            buffer += chunk.content
            placeholder.markdown(render_message_html("assistant", avatar, buffer), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    if buffer and "conversation_log" in st.session_state:
        st.session_state.conversation_log.append({
            "role": "assistant",
            "avatar": avatar,
            "content": buffer
        })
    return buffer


def agent_solution_architect(
    market_data: str,
    internal_data: str,
    interview_memo: str,
    feedback: Optional[str] = None,
    model_name: str = "gemini-2.5-flash-lite",
) -> str:
    """ğŸ’¡ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã¨ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦ææ¡ˆã‚’ä½œæˆã€‚"""

    llm = get_llm(temperature=0.9, streaming=True, model_name=model_name)

    intro = ""
    if feedback:
        intro = "I will refine the plan based on the feedback and ensure the issues are addressed.\n\n"
        # æ—¥æœ¬èªè¨³:
        # ã€Œãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    prompt = (
        "You are a Genius Solution Architect in a chemical company. Combine the following "
        "\"Internal Data\" and \"Market Facts\" to solve the \"Customer Dilemma\" described in the Interview Memo.\n\n"
        "Constraints:\n"
        "Do NOT just propose existing products. Create a \"Chemical Reaction\" (new combination).\n"
        "If feedback is provided, you MUST revise your proposal to address the criticism.\n"
        "Respond in Japanese only.\n\n"
        f"Internal Data:\n{internal_data}\n\n"
        f"Market Facts:\n{market_data}\n\n"
        f"Interview Memo (Customer Dilemma):\n{interview_memo}\n\n"
        f"Feedback (if any):\n{feedback or 'None'}\n\n"
        f"{intro}Respond with a concrete proposal."
    )
    # æ—¥æœ¬èªè¨³:
    # ã€Œã‚ãªãŸã¯åŒ–å­¦ãƒ¡ãƒ¼ã‚«ãƒ¼ã®å¤©æ‰ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã€Internal Dataã€ã¨ã€Market Factsã€ã‚’çµ„ã¿åˆã‚ã›ã€
    # Interview Memo ã«è¨˜è¼‰ã•ã‚ŒãŸã€Customer Dilemmaã€ã‚’è§£æ±ºã™ã‚‹ææ¡ˆã‚’ä½œã£ã¦ãã ã•ã„ã€‚æ—¢å­˜å“ã®ææ¡ˆã ã‘ã¯é¿ã‘ã€
    # æ–°ã—ã„ã€Chemical Reactionï¼ˆçµ„ã¿åˆã‚ã›ï¼‰ã€ã‚’ä½œã‚‹ã“ã¨ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    # æ–°ã—ã„ã€Chemical Reactionï¼ˆçµ„ã¿åˆã‚ã›ï¼‰ã€ã‚’ä½œã‚‹ã“ã¨ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    return _stream_response(llm, [HumanMessage(content=prompt)], avatar=SOLUTION_ARCHITECT_AVATAR)



def agent_devils_advocate(proposal: str, model_name: str = "gemini-2.5-flash-lite") -> str:
    """ğŸ‘¿æ‚ªé­”ã®æ“è­·è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚ææ¡ˆã‚’å³ã—ãæ‰¹åˆ¤ã€‚"""

    llm = get_llm(temperature=0.5, streaming=True, model_name=model_name)
    prompt = (
        "You are a Devil's Advocate (Strict Technical Reviewer) inside the proposing company. "
        "Write as an internal reviewer (use ã€Œå½“ç¤¾ã€ã€Œå½“æ–¹ã€ã€Œæˆ‘ã€…ã€) and never from the client's perspective "
        "(avoid ã€Œè²´ç¤¾/å¾¡ç¤¾ã€ã€ŒãŠå®¢æ§˜ã€ç­‰). Criticize the following proposal ruthlessly. Focus on:\n\n"
        "Chemical Risks (Hydrolysis, Heat degradation)\n"
        "Cost Feasibility\n"
        "Mass Production Issues\n\n"
        "Respond in Japanese only, concise bullet style if suitable.\n\n"
        f"Proposal: {proposal}"
    )
    # æ—¥æœ¬èªè¨³:
    # ã€Œã‚ãªãŸã¯æ‚ªé­”ã®æ“è­·è€…ï¼ˆå³ã—ã„æŠ€è¡“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰ã§ã™ã€‚ä»¥ä¸‹ã®ææ¡ˆã‚’å³ã—ãæ‰¹åˆ¤ã—ã¦ãã ã•ã„ã€‚ç„¦ç‚¹ã¯ï¼š
    # åŒ–å­¦ãƒªã‚¹ã‚¯ï¼ˆæ°´è§£ã€ç†±åŠ£åŒ–ï¼‰
    # ã‚³ã‚¹ãƒˆå®Ÿç¾æ€§
    # é‡ç”£å•é¡Œã§ã™ã€‚ã€

    # åŒ–å­¦ãƒªã‚¹ã‚¯ï¼ˆæ°´è§£ã€ç†±åŠ£åŒ–ï¼‰
    # ã‚³ã‚¹ãƒˆå®Ÿç¾æ€§
    # é‡ç”£å•é¡Œã§ã™ã€‚ã€

    return _stream_response(llm, [HumanMessage(content=prompt)], avatar=DEVILS_ADVOCATE_AVATAR)


def agent_orchestrator_summary(
    proposal: str,
    market_data: str,
    internal_data: str,
    interview_memo: str,
    tech_tags: List[str],
    company_name: str,
    model_name: str = "gemini-2.5-flash-lite",
) -> str:
    """ğŸ‘‘è¦ç´„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚æŒ‡å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ²¿ã£ã¦æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã€‚"""

    llm = get_llm(temperature=0.5, model_name=model_name)

    # /services/report_generator.pyã®REPORT_SYSTEM_PROMPTã‚’ä½¿ç”¨
    system_prompt = REPORT_SYSTEM_PROMPT

    # /services/report_generator.pyã®REPORT_HUMAN_PROMPTã‚’ä½¿ç”¨
    human_prompt = REPORT_HUMAN_PROMPT.format(
        company_name=company_name,
        interview_content=interview_memo,
        tech_tags="ã€".join(tech_tags),
        cross_link_text=internal_data,
        market_trends=market_data,
        proposal=proposal
    )

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=human_prompt),
    ]

    response = llm.invoke(messages)
    summary = response.content.strip()
    return summary


def run_innovation_squad(
    interview_memo: str,
    tech_tags: List[str],
    department: str,
    company_name: str = "",
    progress_callback: Optional[callable] = None,
    model_name: str = "gemini-2.5-flash-lite",
) -> tuple[str, List[dict], List[dict]]:
    """ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†éšŠã®ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã—ã€æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®Markdownã€ä»–äº‹æ¥­éƒ¨çŸ¥è¦‹ãƒªã‚¹ãƒˆã€å­¦è¡“è«–æ–‡æƒ…å ±ã‚’è¿”ã™ã€‚
    
    Returns:
        tuple[str, List[dict], List[dict]]: (æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ, ä»–äº‹æ¥­éƒ¨çŸ¥è¦‹ãƒªã‚¹ãƒˆ, å­¦è¡“è«–æ–‡æƒ…å ±ãƒªã‚¹ãƒˆ)
    """
    # ä¼šè©±ãƒ­ã‚°ã‚’åˆæœŸåŒ–
    if "conversation_log" not in st.session_state:
        st.session_state.conversation_log = []
    
    # CSSã¯å‘¼ã³å‡ºã—å…ƒã§æ³¨å…¥æ¸ˆã¿ã®ãŸã‚å‰Šé™¤
    # st.markdown(get_chat_css(), unsafe_allow_html=True)

    if progress_callback:
        progress_callback(15, "ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼: ãƒãƒ¼ãƒ ã¸ã®ãƒ–ãƒªãƒ¼ãƒ•ã‚£ãƒ³ã‚°ã‚’ä½œæˆä¸­...")

    brief = generate_orchestrator_brief(interview_memo, model_name=model_name)
    brief_content = brief or "ãƒãƒ¼ãƒ ã€é–‹å§‹ã—ã¾ã—ã‚‡ã†ã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, brief_content), unsafe_allow_html=True)
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": brief_content
    })

    if progress_callback:
        progress_callback(30, "ãƒãƒ¼ã‚±ãƒƒãƒˆãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ & ç¤¾å†…ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆ: æƒ…å ±åé›†ä¸­...")

    market_data, academic_results = agent_market_researcher(tech_tags, use_case=interview_memo, model_name=model_name)
    internal_data, internal_hits = agent_internal_specialist(interview_memo, department)

    if progress_callback:
        progress_callback(40, "ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼: è­°è«–ã®æ–¹å‘æ€§ã‚’æŒ‡ç¤ºä¸­...")

    orchestrator_msg1 = "ææ–™ã¯æƒã£ãŸã€‚Architectã€ç«¶åˆã‚’ä¸Šå›ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ„ã‚“ã§ãã‚Œã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, orchestrator_msg1), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": orchestrator_msg1
    })
    
    if progress_callback:
        progress_callback(55, "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ: åˆæœŸææ¡ˆã‚’ä½œæˆä¸­...")

    proposal_v1 = agent_solution_architect(market_data, internal_data, interview_memo, model_name=model_name)
    # ä¼šè©±ãƒ­ã‚°ã¯agent_solution_architectå†…ã®_stream_responseã§è¿½åŠ æ¸ˆã¿

    if progress_callback:
        progress_callback(70, "ãƒ‡ãƒ“ãƒ«ã‚ºã‚¢ãƒ‰ãƒœã‚±ã‚¤ãƒˆ: ãƒªã‚¹ã‚¯åˆ†æã¨æ‰¹åˆ¤çš„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿè¡Œä¸­...")

    orchestrator_msg2 = "Devilã€ã“ã®æ¡ˆã®å¼±ç‚¹ã‚’æ´—ã„å‡ºã—ã¦ãã‚Œã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, orchestrator_msg2), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": orchestrator_msg2
    })
    
    critique = agent_devils_advocate(proposal_v1, model_name=model_name)
    # ä¼šè©±ãƒ­ã‚°ã¯agent_devils_advocateå†…ã®_stream_responseã§è¿½åŠ æ¸ˆã¿

    if progress_callback:
        progress_callback(80, "ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼: æ”¹å–„æŒ‡ç¤ºã‚’å‡ºã—ã¦ã„ã¾ã™...")

    orchestrator_msg3 = "Architectã€æŒ‡æ‘˜ã‚’è¸ã¾ãˆã¦æ”¹è¨‚æ¡ˆã‚’å‡ºã—ã¦ã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, orchestrator_msg3), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": orchestrator_msg3
    })
    
    if progress_callback:
        progress_callback(90, "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ: æœ€çµ‚ææ¡ˆã‚’ç·´ã‚Šä¸Šã’ã¦ã„ã¾ã™...")

    proposal_final = agent_solution_architect(market_data, internal_data, interview_memo, feedback=critique, model_name=model_name)
    # ä¼šè©±ãƒ­ã‚°ã¯agent_solution_architectå†…ã®_stream_responseã§è¿½åŠ æ¸ˆã¿

    orchestrator_msg4 = "ã‚ˆã—ã€ã“ã‚Œã§è¡Œã“ã†ï¼ ã¿ã‚“ãªã‚ã‚ŠãŒã¨ã†ã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, orchestrator_msg4), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": orchestrator_msg4
    })

    if progress_callback:
        progress_callback(95, "ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼: æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­...")

    final_report_md = agent_orchestrator_summary(
        proposal=proposal_final,
        market_data=market_data,
        internal_data=internal_data,
        interview_memo=interview_memo,
        tech_tags=tech_tags,
        company_name=company_name,
        model_name=model_name,
    )
    # ä¼šè©±ãƒ­ã‚°ã¯agent_orchestrator_summaryå†…ã§è¿½åŠ æ¸ˆã¿
    
    if progress_callback:
        progress_callback(100, "å®Œäº†ï¼")

    return final_report_md, internal_hits, academic_results
