"""ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†éšŠï¼ˆ5ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰ã®ä¸­æ ¸ãƒ­ã‚¸ãƒƒã‚¯ã€‚

Streamlitã®ãƒãƒ£ãƒƒãƒˆUIã§5äººãŒè­°è«–ã™ã‚‹ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨é€²è¡Œã¯ä»•æ§˜ã«å¾“ã†ã€‚
"""

import os
from typing import List, Optional, Dict

import streamlit as st
from langchain_core.messages import HumanMessage, SystemMessage

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    ChatGoogleGenerativeAI = None

import backend
from services.patents import search_patents
from services.academic import search_arxiv, format_arxiv_results
from services.ai_review import select_important_tags

from services.report_generator import REPORT_SYSTEM_PROMPT, REPORT_HUMAN_PROMPT
from components.conversation_log import get_chat_css, render_message_html

# å®šæ•°å®šç¾©
ORCHESTRATOR_AVATAR = "/Users/ayu/create/AgentX2/R-D_AIagent_MVP/images/Orchestrator.png"
MARKET_RESEARCHER_AVATAR = "/Users/ayu/create/AgentX2/R-D_AIagent_MVP/images/Market_Researcher.png"
INTERNAL_SPECIALIST_AVATAR = "/Users/ayu/create/AgentX2/R-D_AIagent_MVP/images/Internal_Specialist.png"
SOLUTION_ARCHITECT_AVATAR = "/Users/ayu/create/AgentX2/R-D_AIagent_MVP/images/Solution_Architect.png"
DEVILS_ADVOCATE_AVATAR = "/Users/ayu/create/AgentX2/R-D_AIagent_MVP/images/Devils_Advocate.png"


def get_llm(temperature: float = 0.3, streaming: bool = False):
    """LLMã‚’è¿”ã™ãƒ•ã‚¡ã‚¯ãƒˆãƒªã€‚Gemini 2.5 Flash ã‚’ä½¿ç”¨ã€‚"""

    if ChatGoogleGenerativeAI is None:
        raise ImportError("Gemini ã‚’ä½¿ã†ã«ã¯ langchain-google-genai ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™")

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    return ChatGoogleGenerativeAI(
        # model="gemini-2.5-flash",
        model="gemini-2.5-flash-lite",
        temperature=temperature,
        google_api_key=api_key,
        streaming=streaming,
    )


def generate_orchestrator_brief(interview_memo: str) -> str:
    """ğŸ‘‘å¸ä¼šç”¨ã®çŸ­ã„ãƒ–ãƒªãƒ¼ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    llm = get_llm(temperature=0.5)
    prompt = (
        "ã‚ãªãŸã¯ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®é¢è«‡ãƒ¡ãƒ¢ã‚’èª­ã¿ã€1æ®µè½ã§å¸ä¼šç”¨ãƒ–ãƒªãƒ¼ãƒ•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        "å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
        "å«ã‚ã‚‹è¦ç´ : ä¸»èª²é¡Œ/è¦æ±‚ã‚¹ãƒšãƒƒã‚¯ã€ç«¶åˆãƒ»ææ–™ã®å€™è£œã€ä¸»è¦ãƒªã‚¹ã‚¯ã€ç´æœŸãŒã‚ã‚Œã°æ˜ç¤ºã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®æŒ‡ç¤º"
        " (Market=äº‹å®Ÿèª¿æŸ», Internal=ç¤¾å†…çŸ¥è¦‹, Architect=ç™ºæƒ³, Devil=ãƒªã‚¹ã‚¯ç¢ºèª)ã€‚"
        "æœ€åˆã®è¡Œã«ãƒ¡ã‚¿æƒ…å ±ã‚’æ›¸ã„ã¦ãã ã•ã„: [meta role=assistant tokens=<æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°>]. "
        "æœ¬æ–‡ã¯ãã®æ¬¡ã®è¡Œã‹ã‚‰æ›¸ãã€200æ–‡å­—ã‚’è¶…ãˆãã†ãªã‚‰å¥ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼‰ã®ç›´å¾Œã« `--- SPLIT ---` ã‚’æŒ¿å…¥ã—ã¦ç¶šãã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"
        f"\n\né¢è«‡ãƒ¡ãƒ¢:\n{interview_memo}"
    )
    response = llm.invoke([HumanMessage(content=prompt)])
    return response.content.strip()



def agent_market_researcher(tech_tags: List[str], use_case: str = "") -> tuple[str, List[Dict]]:
    """ğŸ•µï¸å¸‚å ´èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚DuckDuckGo ã§å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œç´¢ã€‚
    
    Returns:
        tuple[str, List[Dict]]: (å¸‚å ´èª¿æŸ»ã‚µãƒãƒªãƒ¼, å­¦è¡“è«–æ–‡æƒ…å ±ã®ãƒªã‚¹ãƒˆ)
    """

    # é‡è¦åº¦ã®é«˜ã„ã‚¿ã‚°ã‚’é¸å®šï¼ˆæœ€å¤§5ã¤ï¼‰
    selected_tags = select_important_tags(tech_tags, interview_memo=use_case, max_tags=5)
    
    # é¸å®šã•ã‚ŒãŸã‚¿ã‚°ã§æ¤œç´¢ã‚’å®Ÿè¡Œ
    results = backend.search_market_trends(selected_tags, use_case) or ""
    patents = search_patents(selected_tags) or ""
    academics_list = search_arxiv(" ".join(selected_tags))
    academics = format_arxiv_results(academics_list) if academics_list else ""
    academics = format_arxiv_results(academics_list) if academics_list else ""
    avatar = MARKET_RESEARCHER_AVATAR
    if not any([results.strip(), patents, academics]):
        summary = "å¸‚å ´ãƒ»ç‰¹è¨±ãƒ»å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        st.markdown(render_message_html("assistant", avatar, summary), unsafe_allow_html=True)
        # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
        if "conversation_log" in st.session_state:
            st.session_state.conversation_log.append({
                "role": "assistant",
                "avatar": avatar,
                "content": summary
            })
        return summary, []

    prompt = (
        "You are a Market Researcher. Summarize the following search results into facts only "
        "(Competitors, Market Size, Trends, Patents, Academic papers). No speculation. "
        "Respond in Japanese only.\n"
        "æœ€åˆã®è¡Œã«ãƒ¡ã‚¿æƒ…å ±ã‚’æ›¸ã„ã¦ãã ã•ã„: [meta role=assistant tokens=<æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°>]. æœ¬æ–‡ã¯2è¡Œç›®ä»¥é™ã«æ›¸ã„ã¦ãã ã•ã„ã€‚\n"
        "å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…ãšè¦‹å‡ºã—è¡Œã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„: '## ç«¶åˆä»–ç¤¾', '## å¸‚å ´è¦æ¨¡', '## ãƒˆãƒ¬ãƒ³ãƒ‰', '## ç‰¹è¨±', '## å­¦è¡“è«–æ–‡'.\n"
        "1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒ2000æ–‡å­—ã‚’è¶…ãˆãã†ãªã‚‰ã€å¥ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼‰ã®ç›´å¾Œã« `--- SPLIT ---` ã‚’æŒ¿å…¥ã—ã¦ç¶šãã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n"
        "1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n"
        "Market: {results}\n\n"
        "Patents: {patents}\n\n"
        "Academic: {academics}"
        # æ—¥æœ¬èªè¨³:
        # ã€Œã‚ãªãŸã¯å¸‚å ´èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’è¦ç´„ã—ã¦ã€ç«¶åˆã€å¸‚å ´ã‚µã‚¤ã‚ºã€ãƒˆãƒ¬ãƒ³ãƒ‰ã€ç‰¹è¨±ã€è«–æ–‡ã‚’äº‹å®Ÿã®ã¿ã§æ›¸ã„ã¦ãã ã•ã„ã€‚æ¨æ¸¬ã¯ã—ãªã„ã§ãã ã•ã„ã€‚ã€
    ).format(results=results, patents=patents, academics=academics)
    llm = get_llm(temperature=0.3)
    response = llm.invoke([HumanMessage(content=prompt)])
    summary = response.content.strip()
    st.markdown(render_message_html("assistant", avatar, summary), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    if "conversation_log" in st.session_state:
        st.session_state.conversation_log.append({
            "role": "assistant",
            "avatar": avatar,
            "content": summary
        })
    return summary, academics_list



def agent_internal_specialist(query_text: str, department: str) -> tuple[str, List[dict]]:
    """ğŸ”ç¤¾å†…ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚ä»–äº‹æ¥­éƒ¨ã®çŸ¥è¦‹ã‚’æ¤œç´¢ã€‚"""

    hits = backend.search_cross_pollination(query_text, department, top_k=3) or []
    hits = backend.search_cross_pollination(query_text, department, top_k=3) or []
    avatar = INTERNAL_SPECIALIST_AVATAR
    if not hits:
        msg = "é–¢é€£ã™ã‚‹ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        st.markdown(render_message_html("assistant", avatar, msg), unsafe_allow_html=True)
        # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
        if "conversation_log" in st.session_state:
            st.session_state.conversation_log.append({
                "role": "assistant",
                "avatar": avatar,
                "content": msg
            })
        return msg, []

    bullet_lines = []
    for item in hits:
        metadata = item.get("metadata", {}) if isinstance(item, dict) else {}
        company = metadata.get("company") or metadata.get("client") or "Unknown Company"
        dept = metadata.get("department") or "Unknown Dept"
        content = item.get("content", "") if isinstance(item, dict) else ""
        bullet_lines.append(f"- {company} ({dept}): {content[:200]}".strip())

    result_text = "\n".join(bullet_lines)
    st.markdown(render_message_html("assistant", avatar, result_text), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    if "conversation_log" in st.session_state:
        st.session_state.conversation_log.append({
            "role": "assistant",
            "avatar": avatar,
            "content": result_text
        })
    return result_text, hits



def _stream_response(llm, messages: List, avatar: str) -> str:
    """LLMå‡ºåŠ›ã‚’Streamlitã«ã‚¹ãƒˆãƒªãƒ¼ãƒ è¡¨ç¤ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚"""

    buffer = ""
    placeholder = st.empty()
    for chunk in llm.stream(messages):
        if chunk.content:
            buffer += chunk.content
            placeholder.markdown(render_message_html("assistant", avatar, buffer), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    if buffer and "conversation_log" in st.session_state:
        st.session_state.conversation_log.append({
            "role": "assistant",
            "avatar": avatar,
            "content": buffer
        })
    return buffer


def agent_solution_architect(
    market_data: str,
    internal_data: str,
    interview_memo: str,
    feedback: Optional[str] = None,
) -> str:
    """ğŸ’¡ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã¨ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦ææ¡ˆã‚’ä½œæˆã€‚"""

    llm = get_llm(temperature=0.9, streaming=True)

    intro = ""
    if feedback:
        intro = "I will refine the plan based on the feedback and ensure the issues are addressed.\n\n"
        # æ—¥æœ¬èªè¨³:
        # ã€Œãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    prompt = (
        "You are a Genius Solution Architect in a chemical company. Combine the following "
        "\"Internal Data\" and \"Market Facts\" to solve the \"Customer Dilemma\" described in the Interview Memo.\n\n"
        "Constraints:\n"
        "Do NOT just propose existing products. Create a \"Chemical Reaction\" (new combination).\n"
        "If feedback is provided, you MUST revise your proposal to address the criticism.\n"
        "Respond in Japanese only.\n\n"
        f"Internal Data:\n{internal_data}\n\n"
        f"Market Facts:\n{market_data}\n\n"
        f"Interview Memo (Customer Dilemma):\n{interview_memo}\n\n"
        f"Feedback (if any):\n{feedback or 'None'}\n\n"
        f"{intro}Respond with a concrete proposal."
    )
    # æ—¥æœ¬èªè¨³:
    # ã€Œã‚ãªãŸã¯åŒ–å­¦ãƒ¡ãƒ¼ã‚«ãƒ¼ã®å¤©æ‰ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã€Internal Dataã€ã¨ã€Market Factsã€ã‚’çµ„ã¿åˆã‚ã›ã€
    # Interview Memo ã«è¨˜è¼‰ã•ã‚ŒãŸã€Customer Dilemmaã€ã‚’è§£æ±ºã™ã‚‹ææ¡ˆã‚’ä½œã£ã¦ãã ã•ã„ã€‚æ—¢å­˜å“ã®ææ¡ˆã ã‘ã¯é¿ã‘ã€
    # æ–°ã—ã„ã€Chemical Reactionï¼ˆçµ„ã¿åˆã‚ã›ï¼‰ã€ã‚’ä½œã‚‹ã“ã¨ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    # æ–°ã—ã„ã€Chemical Reactionï¼ˆçµ„ã¿åˆã‚ã›ï¼‰ã€ã‚’ä½œã‚‹ã“ã¨ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    return _stream_response(llm, [HumanMessage(content=prompt)], avatar=SOLUTION_ARCHITECT_AVATAR)



def agent_devils_advocate(proposal: str) -> str:
    """ğŸ‘¿æ‚ªé­”ã®æ“è­·è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚ææ¡ˆã‚’å³ã—ãæ‰¹åˆ¤ã€‚"""

    llm = get_llm(temperature=0.5, streaming=True)
    prompt = (
        "You are a Devil's Advocate (Strict Technical Reviewer) inside the proposing company. "
        "Write as an internal reviewer (use ã€Œå½“ç¤¾ã€ã€Œå½“æ–¹ã€ã€Œæˆ‘ã€…ã€) and never from the client's perspective "
        "(avoid ã€Œè²´ç¤¾/å¾¡ç¤¾ã€ã€ŒãŠå®¢æ§˜ã€ç­‰). Criticize the following proposal ruthlessly. Focus on:\n\n"
        "Chemical Risks (Hydrolysis, Heat degradation)\n"
        "Cost Feasibility\n"
        "Mass Production Issues\n\n"
        "Respond in Japanese only, concise bullet style if suitable.\n\n"
        f"Proposal: {proposal}"
    )
    # æ—¥æœ¬èªè¨³:
    # ã€Œã‚ãªãŸã¯æ‚ªé­”ã®æ“è­·è€…ï¼ˆå³ã—ã„æŠ€è¡“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰ã§ã™ã€‚ä»¥ä¸‹ã®ææ¡ˆã‚’å³ã—ãæ‰¹åˆ¤ã—ã¦ãã ã•ã„ã€‚ç„¦ç‚¹ã¯ï¼š
    # åŒ–å­¦ãƒªã‚¹ã‚¯ï¼ˆæ°´è§£ã€ç†±åŠ£åŒ–ï¼‰
    # ã‚³ã‚¹ãƒˆå®Ÿç¾æ€§
    # é‡ç”£å•é¡Œã§ã™ã€‚ã€

    # åŒ–å­¦ãƒªã‚¹ã‚¯ï¼ˆæ°´è§£ã€ç†±åŠ£åŒ–ï¼‰
    # ã‚³ã‚¹ãƒˆå®Ÿç¾æ€§
    # é‡ç”£å•é¡Œã§ã™ã€‚ã€

    return _stream_response(llm, [HumanMessage(content=prompt)], avatar=DEVILS_ADVOCATE_AVATAR)


def agent_orchestrator_summary(
    proposal: str,
    market_data: str,
    internal_data: str,
    interview_memo: str,
    tech_tags: List[str],
    company_name: str,
) -> str:
    """ğŸ‘‘è¦ç´„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚æŒ‡å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ²¿ã£ã¦æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã€‚"""

    llm = get_llm(temperature=0.5)

    # /services/report_generator.pyã®REPORT_SYSTEM_PROMPTã‚’ä½¿ç”¨
    system_prompt = REPORT_SYSTEM_PROMPT

    # /services/report_generator.pyã®REPORT_HUMAN_PROMPTã‚’ä½¿ç”¨
    human_prompt = REPORT_HUMAN_PROMPT.format(
        company_name=company_name,
        interview_content=interview_memo,
        tech_tags="ã€".join(tech_tags),
        cross_link_text=internal_data,
        market_trends=market_data,
        proposal=proposal
    )

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=human_prompt),
    ]

    response = llm.invoke(messages)
    summary = response.content.strip()
    return summary


def run_innovation_squad(
    interview_memo: str,
    tech_tags: List[str],
    department: str,
    company_name: str = "",
) -> tuple[str, List[dict], List[dict]]:
    """ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†éšŠã®ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã—ã€æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®Markdownã€ä»–äº‹æ¥­éƒ¨çŸ¥è¦‹ãƒªã‚¹ãƒˆã€å­¦è¡“è«–æ–‡æƒ…å ±ã‚’è¿”ã™ã€‚
    
    Returns:
        tuple[str, List[dict], List[dict]]: (æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ, ä»–äº‹æ¥­éƒ¨çŸ¥è¦‹ãƒªã‚¹ãƒˆ, å­¦è¡“è«–æ–‡æƒ…å ±ãƒªã‚¹ãƒˆ)
    """
    # ä¼šè©±ãƒ­ã‚°ã‚’åˆæœŸåŒ–
    if "conversation_log" not in st.session_state:
        st.session_state.conversation_log = []
    
    # CSSã‚’æ³¨å…¥
    st.markdown(get_chat_css(), unsafe_allow_html=True)

    brief = generate_orchestrator_brief(interview_memo)
    brief_content = brief or "ãƒãƒ¼ãƒ ã€é–‹å§‹ã—ã¾ã—ã‚‡ã†ã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, brief_content), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": brief_content
    })

    market_data, academic_results = agent_market_researcher(tech_tags, use_case=interview_memo)
    internal_data, internal_hits = agent_internal_specialist(interview_memo, department)

    orchestrator_msg1 = "ææ–™ã¯æƒã£ãŸã€‚Architectã€ç«¶åˆã‚’ä¸Šå›ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ„ã‚“ã§ãã‚Œã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, orchestrator_msg1), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": orchestrator_msg1
    })
    
    proposal_v1 = agent_solution_architect(market_data, internal_data, interview_memo)
    # ä¼šè©±ãƒ­ã‚°ã¯agent_solution_architectå†…ã®_stream_responseã§è¿½åŠ æ¸ˆã¿

    orchestrator_msg2 = "Devilã€ã“ã®æ¡ˆã®å¼±ç‚¹ã‚’æ´—ã„å‡ºã—ã¦ãã‚Œã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, orchestrator_msg2), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": orchestrator_msg2
    })
    
    critique = agent_devils_advocate(proposal_v1)
    # ä¼šè©±ãƒ­ã‚°ã¯agent_devils_advocateå†…ã®_stream_responseã§è¿½åŠ æ¸ˆã¿

    orchestrator_msg3 = "Architectã€æŒ‡æ‘˜ã‚’è¸ã¾ãˆã¦æ”¹è¨‚æ¡ˆã‚’å‡ºã—ã¦ã€‚"
    st.markdown(render_message_html("assistant", ORCHESTRATOR_AVATAR, orchestrator_msg3), unsafe_allow_html=True)
    # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
    st.session_state.conversation_log.append({
        "role": "assistant",
        "avatar": ORCHESTRATOR_AVATAR,
        "content": orchestrator_msg3
    })
    
    proposal_final = agent_solution_architect(market_data, internal_data, interview_memo, feedback=critique)
    # ä¼šè©±ãƒ­ã‚°ã¯agent_solution_architectå†…ã®_stream_responseã§è¿½åŠ æ¸ˆã¿

    final_report_md = agent_orchestrator_summary(
        proposal=proposal_final,
        market_data=market_data,
        internal_data=internal_data,
        interview_memo=interview_memo,
        tech_tags=tech_tags,
        company_name=company_name,
    )
    # ä¼šè©±ãƒ­ã‚°ã¯agent_orchestrator_summaryå†…ã§è¿½åŠ æ¸ˆã¿
    
    return final_report_md, internal_hits, academic_results
