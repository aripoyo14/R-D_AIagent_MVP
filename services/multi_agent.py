"""ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†éšŠï¼ˆ5ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰ã®ä¸­æ ¸ãƒ­ã‚¸ãƒƒã‚¯ã€‚

Streamlitã®ãƒãƒ£ãƒƒãƒˆUIã§5äººãŒè­°è«–ã™ã‚‹ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨é€²è¡Œã¯ä»•æ§˜ã«å¾“ã†ã€‚
"""

import os
from typing import List, Optional

import streamlit as st
from langchain_core.messages import HumanMessage, SystemMessage

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    ChatGoogleGenerativeAI = None

import backend
from services.patents import search_patents
from services.academic import search_arxiv

from services.report_generator import REPORT_SYSTEM_PROMPT, REPORT_HUMAN_PROMPT


def get_llm(temperature: float = 0.3, streaming: bool = False):
    """LLMã‚’è¿”ã™ãƒ•ã‚¡ã‚¯ãƒˆãƒªã€‚Gemini 2.5 Flash ã‚’ä½¿ç”¨ã€‚"""

    if ChatGoogleGenerativeAI is None:
        raise ImportError("Gemini ã‚’ä½¿ã†ã«ã¯ langchain-google-genai ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™")

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=temperature,
        google_api_key=api_key,
        streaming=streaming,
    )


def generate_orchestrator_brief(interview_memo: str) -> str:
    """ğŸ‘‘å¸ä¼šç”¨ã®çŸ­ã„ãƒ–ãƒªãƒ¼ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    llm = get_llm(temperature=0.5)
    prompt = (
        "ã‚ãªãŸã¯ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®é¢è«‡ãƒ¡ãƒ¢ã‚’èª­ã¿ã€1æ®µè½ã§å¸ä¼šç”¨ãƒ–ãƒªãƒ¼ãƒ•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        "å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
        "å«ã‚ã‚‹è¦ç´ : ä¸»èª²é¡Œ/è¦æ±‚ã‚¹ãƒšãƒƒã‚¯ã€ç«¶åˆãƒ»ææ–™ã®å€™è£œã€ä¸»è¦ãƒªã‚¹ã‚¯ã€ç´æœŸãŒã‚ã‚Œã°æ˜ç¤ºã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®æŒ‡ç¤º"
        " (Market=äº‹å®Ÿèª¿æŸ», Internal=ç¤¾å†…çŸ¥è¦‹, Architect=ç™ºæƒ³, Devil=ãƒªã‚¹ã‚¯ç¢ºèª)ã€‚\n\n"
        f"é¢è«‡ãƒ¡ãƒ¢:\n{interview_memo}"
    )
    response = llm.invoke([HumanMessage(content=prompt)])
    return response.content.strip()



def agent_market_researcher(tech_tags: List[str], use_case: str = "") -> str:
    """ğŸ•µï¸å¸‚å ´èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚DuckDuckGo ã§å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œç´¢ã€‚"""

    results = backend.search_market_trends(tech_tags, use_case) or ""
    patents = search_patents(" ".join(tech_tags)) or ""
    academics = search_arxiv(" ".join(tech_tags)) or ""
    avatar = "ğŸ•µï¸"
    with st.chat_message("assistant", avatar=avatar):
        if not any([results.strip(), patents, academics]):
            st.markdown("No market/patent/academic data found.")
            return "No market/patent/academic data found."

        prompt = (
            "You are a Market Researcher. Summarize the following search results into facts only "
            "(Competitors, Market Size, Trends, Patents, Academic papers). No speculation. "
            "Respond in Japanese only.\n\n"
            "Market: {results}\n\n"
            "Patents: {patents}\n\n"
            "Academic: {academics}"
            # æ—¥æœ¬èªè¨³:
            # ã€Œã‚ãªãŸã¯å¸‚å ´èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’è¦ç´„ã—ã¦ã€ç«¶åˆã€å¸‚å ´ã‚µã‚¤ã‚ºã€ãƒˆãƒ¬ãƒ³ãƒ‰ã€ç‰¹è¨±ã€è«–æ–‡ã‚’äº‹å®Ÿã®ã¿ã§æ›¸ã„ã¦ãã ã•ã„ã€‚æ¨æ¸¬ã¯ã—ãªã„ã§ãã ã•ã„ã€‚ã€
        ).format(results=results, patents=patents, academics=academics)
        llm = get_llm(temperature=0.3)
        response = llm.invoke([HumanMessage(content=prompt)])
        summary = response.content.strip()
        st.markdown(summary)
        return summary



def agent_internal_specialist(query_text: str, department: str) -> tuple[str, List[dict]]:
    """ğŸ”ç¤¾å†…ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚ä»–äº‹æ¥­éƒ¨ã®çŸ¥è¦‹ã‚’æ¤œç´¢ã€‚"""

    hits = backend.search_cross_pollination(query_text, department, top_k=3) or []
    avatar = "ğŸ”"
    with st.chat_message("assistant", avatar=avatar):
        if not hits:
            msg = "No relevant internal data found."
            st.markdown(msg)
            return msg, []

        bullet_lines = []
        for item in hits:
            metadata = item.get("metadata", {}) if isinstance(item, dict) else {}
            company = metadata.get("company") or metadata.get("client") or "Unknown Company"
            dept = metadata.get("department") or "Unknown Dept"
            content = item.get("content", "") if isinstance(item, dict) else ""
            bullet_lines.append(f"- {company} ({dept}): {content[:200]}".strip())

        result_text = "\n".join(bullet_lines)
        st.markdown(result_text)
        return result_text, hits



def _stream_response(llm, messages: List, avatar: str) -> str:
    """LLMå‡ºåŠ›ã‚’Streamlitã«ã‚¹ãƒˆãƒªãƒ¼ãƒ è¡¨ç¤ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚"""

    buffer = ""
    with st.chat_message("assistant", avatar=avatar):
        placeholder = st.empty()
        for chunk in llm.stream(messages):
            if chunk.content:
                buffer += chunk.content
                placeholder.markdown(buffer)
    return buffer


def agent_solution_architect(
    market_data: str,
    internal_data: str,
    interview_memo: str,
    feedback: Optional[str] = None,
) -> str:
    """ğŸ’¡ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã¨ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦ææ¡ˆã‚’ä½œæˆã€‚"""

    llm = get_llm(temperature=0.9, streaming=True)

    intro = ""
    if feedback:
        intro = "I will refine the plan based on the feedback and ensure the issues are addressed.\n\n"
        # æ—¥æœ¬èªè¨³:
        # ã€Œãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    prompt = (
        "You are a Genius Solution Architect in a chemical company. Combine the following "
        "\"Internal Data\" and \"Market Facts\" to solve the \"Customer Dilemma\" described in the Interview Memo.\n\n"
        "Constraints:\n"
        "Do NOT just propose existing products. Create a \"Chemical Reaction\" (new combination).\n"
        "If feedback is provided, you MUST revise your proposal to address the criticism.\n"
        "Respond in Japanese only.\n\n"
        f"Internal Data:\n{internal_data}\n\n"
        f"Market Facts:\n{market_data}\n\n"
        f"Interview Memo (Customer Dilemma):\n{interview_memo}\n\n"
        f"Feedback (if any):\n{feedback or 'None'}\n\n"
        f"{intro}Respond with a concrete proposal."
    )
    # æ—¥æœ¬èªè¨³:
    # ã€Œã‚ãªãŸã¯åŒ–å­¦ãƒ¡ãƒ¼ã‚«ãƒ¼ã®å¤©æ‰ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã€Internal Dataã€ã¨ã€Market Factsã€ã‚’çµ„ã¿åˆã‚ã›ã€
    # Interview Memo ã«è¨˜è¼‰ã•ã‚ŒãŸã€Customer Dilemmaã€ã‚’è§£æ±ºã™ã‚‹ææ¡ˆã‚’ä½œã£ã¦ãã ã•ã„ã€‚æ—¢å­˜å“ã®ææ¡ˆã ã‘ã¯é¿ã‘ã€
    # æ–°ã—ã„ã€Chemical Reactionï¼ˆçµ„ã¿åˆã‚ã›ï¼‰ã€ã‚’ä½œã‚‹ã“ã¨ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ææ¡ˆã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã€‚ã€

    return _stream_response(llm, [HumanMessage(content=prompt)], avatar="ğŸ’¡")



def agent_devils_advocate(proposal: str) -> str:
    """ğŸ‘¿æ‚ªé­”ã®æ“è­·è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚ææ¡ˆã‚’å³ã—ãæ‰¹åˆ¤ã€‚"""

    llm = get_llm(temperature=0.5, streaming=True)
    prompt = (
        "You are a Devil's Advocate (Strict Technical Reviewer) inside the proposing company. "
        "Write as an internal reviewer (use ã€Œå½“ç¤¾ã€ã€Œå½“æ–¹ã€ã€Œæˆ‘ã€…ã€) and never from the client's perspective "
        "(avoid ã€Œè²´ç¤¾/å¾¡ç¤¾ã€ã€ŒãŠå®¢æ§˜ã€ç­‰). Criticize the following proposal ruthlessly. Focus on:\n\n"
        "Chemical Risks (Hydrolysis, Heat degradation)\n"
        "Cost Feasibility\n"
        "Mass Production Issues\n\n"
        "Respond in Japanese only, concise bullet style if suitable.\n\n"
        f"Proposal: {proposal}"
    )
    # æ—¥æœ¬èªè¨³:
    # ã€Œã‚ãªãŸã¯æ‚ªé­”ã®æ“è­·è€…ï¼ˆå³ã—ã„æŠ€è¡“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰ã§ã™ã€‚ä»¥ä¸‹ã®ææ¡ˆã‚’å³ã—ãæ‰¹åˆ¤ã—ã¦ãã ã•ã„ã€‚ç„¦ç‚¹ã¯ï¼š
    # åŒ–å­¦ãƒªã‚¹ã‚¯ï¼ˆæ°´è§£ã€ç†±åŠ£åŒ–ï¼‰
    # ã‚³ã‚¹ãƒˆå®Ÿç¾æ€§
    # é‡ç”£å•é¡Œã§ã™ã€‚ã€

    return _stream_response(llm, [HumanMessage(content=prompt)], avatar="ğŸ‘¿")


def agent_orchestrator_summary(
    proposal: str,
    market_data: str,
    internal_data: str,
    interview_memo: str,
    tech_tags: List[str],
    company_name: str,
) -> str:
    """ğŸ‘‘è¦ç´„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚æŒ‡å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ²¿ã£ã¦æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã€‚"""

    llm = get_llm(temperature=0.5)

    # /services/report_generator.pyã®REPORT_SYSTEM_PROMPTã‚’ä½¿ç”¨
    system_prompt = REPORT_SYSTEM_PROMPT

    # /services/report_generator.pyã®REPORT_HUMAN_PROMPTã‚’ä½¿ç”¨
    human_prompt = REPORT_HUMAN_PROMPT.format(
        company_name=company_name,
        interview_content=interview_memo,
        tech_tags="ã€".join(tech_tags),
        cross_link_text=internal_data,
        market_trends=market_data,
        proposal=proposal
    )

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=human_prompt),
    ]

    response = llm.invoke(messages)
    summary = response.content.strip()
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown(summary)
    return summary


def run_innovation_squad(
    interview_memo: str,
    tech_tags: List[str],
    department: str,
    company_name: str = "",
) -> tuple[str, List[dict]]:
    """ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†éšŠã®ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã—ã€æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®Markdownã¨ä»–äº‹æ¥­éƒ¨çŸ¥è¦‹ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚"""

    brief = generate_orchestrator_brief(interview_memo)
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown(brief or "Team, let's start.")

    market_data = agent_market_researcher(tech_tags, use_case=interview_memo)
    internal_data, internal_hits = agent_internal_specialist(interview_memo, department)

    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown("ææ–™ã¯æƒã£ãŸã€‚Architectã€ç«¶åˆã‚’ä¸Šå›ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ„ã‚“ã§ãã‚Œã€‚")
    proposal_v1 = agent_solution_architect(market_data, internal_data, interview_memo)

    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown("Devilã€ã“ã®æ¡ˆã®å¼±ç‚¹ã‚’æ´—ã„å‡ºã—ã¦ãã‚Œã€‚")
    critique = agent_devils_advocate(proposal_v1)

    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown("Architectã€æŒ‡æ‘˜ã‚’è¸ã¾ãˆã¦æ”¹è¨‚æ¡ˆã‚’å‡ºã—ã¦ã€‚")
    proposal_final = agent_solution_architect(market_data, internal_data, interview_memo, feedback=critique)

    final_report_md = agent_orchestrator_summary(
        proposal=proposal_final,
        market_data=market_data,
        internal_data=internal_data,
        interview_memo=interview_memo,
        tech_tags=tech_tags,
        company_name=company_name,
    )
    return final_report_md, internal_hits
